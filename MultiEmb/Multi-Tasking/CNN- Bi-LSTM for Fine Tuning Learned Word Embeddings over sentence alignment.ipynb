{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence to Sequence encoder decorder Bi-LSTM LATER\n",
    "# Multilingual Word Embeddings obtained from the supervised expert dictionary to initialize the embeddings layer \n",
    "# CNN Bidirectional LSTM to encode sentences in source language\n",
    "# CNN Bidirectional LSTM to encode sentences in target language\n",
    "# Take the average of the encoded states using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#from data_helper\n",
    "#from rnn_classifier import rnn_clf\n",
    "#from cnn_classifier import cnn_clf\n",
    "#from clstm_classifier import clstm_clf\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as e:\n",
    "    error = \"Please install scikit-learn.\"\n",
    "    print(str(e) + ': ' + error)\n",
    "    sys.exit()\n",
    "    \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Show warnings and errors only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "batch_size: 32\n",
      "clf: cnn\n",
      "data_file: /Users/meryemmhamdi/Documents/rig9/ParallelCorpora/Europarl/sample_\n",
      "emb_path: /Users/meryemmhamdi/Documents/rig9/MultilingualEmbeddings/sample_emb.txt\n",
      "embedding_size: 300\n",
      "evaluate_every_steps: 100\n",
      "filter_sizes: 3, 4, 5\n",
      "keep_prob: 0.5\n",
      "l2_reg_lambda: 0.001\n",
      "language: en\n",
      "learning_rate: 0.001\n",
      "max_length: 0\n",
      "min_frequency: 0\n",
      "num_checkpoint: 10\n",
      "num_classes: 2\n",
      "num_epochs: 50\n",
      "num_filters: 128\n",
      "save_every_steps: 1000\n",
      "src: fr\n",
      "stop_word_file: None\n",
      "test_size: 0.1\n",
      "trg: en\n",
      "vocab_size: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model choices\n",
    "tf.flags.DEFINE_string('clf', 'cnn', \"Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm]\")\n",
    "\n",
    "# Data parameters\n",
    "tf.flags.DEFINE_string('data_file', \"/Users/meryemmhamdi/Documents/rig9/ParallelCorpora/Europarl/sample_\", 'Data file path')\n",
    "tf.flags.DEFINE_string('emb_path', \"/Users/meryemmhamdi/Documents/rig9/MultilingualEmbeddings/sample_emb.txt\", 'Data file path')\n",
    "tf.flags.DEFINE_string('stop_word_file', None, 'Stop word file path')\n",
    "tf.flags.DEFINE_string('language', 'en', \"Language of the data file. You have two choices: [ch, en]\")\n",
    "tf.flags.DEFINE_integer('min_frequency', 0, 'Minimal word frequency')\n",
    "tf.flags.DEFINE_integer('num_classes', 2, 'Number of classes')\n",
    "tf.flags.DEFINE_integer('max_length', 0, 'Max document length')\n",
    "tf.flags.DEFINE_integer('vocab_size', 0, 'Vocabulary size')\n",
    "tf.flags.DEFINE_float('test_size', 0.1, 'Cross validation test size')\n",
    "\n",
    "# Model hyperparameters\n",
    "tf.flags.DEFINE_integer('embedding_size', 300, 'Word embedding size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_string('filter_sizes', '3, 4, 5', 'CNN filter sizes. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('num_filters', 128, 'Number of filters per filter size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('hidden_size', 128, 'Number of hidden units in the LSTM cell. For LSTM, Bi-LSTM')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'Number of the LSTM cells. For LSTM, Bi-LSTM, C-LSTM')\n",
    "tf.flags.DEFINE_float('keep_prob', 0.5, 'Dropout keep probability')  # All\n",
    "tf.flags.DEFINE_float('learning_rate', 1e-3, 'Learning rate')  # All\n",
    "tf.flags.DEFINE_float('l2_reg_lambda', 0.001, 'L2 regularization lambda')  # All\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer('batch_size', 32, 'Batch size')\n",
    "tf.flags.DEFINE_integer('num_epochs', 50, 'Number of epochs')\n",
    "tf.flags.DEFINE_integer('evaluate_every_steps', 100, 'Evaluate the model on validation set after this many steps')\n",
    "tf.flags.DEFINE_integer('save_every_steps', 1000, 'Save the model after this many steps')\n",
    "tf.flags.DEFINE_integer('num_checkpoint', 10, 'Number of models to store')\n",
    "tf.flags.DEFINE_string('src', \"fr\", 'source language')\n",
    "tf.flags.DEFINE_string('trg', \"en\", 'source language')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "if FLAGS.clf == 'lstm':\n",
    "    FLAGS.embedding_size = FLAGS.hidden_size\n",
    "elif FLAGS.clf == 'clstm':\n",
    "    FLAGS.hidden_size = len(FLAGS.filter_sizes.split(\",\")) * FLAGS.num_filters\n",
    "\n",
    "# Output files directory\n",
    "timestamp = str(int(time.time()))\n",
    "outdir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "\n",
    "params = FLAGS.__flags\n",
    "\n",
    "# Print parameters\n",
    "model = params['clf']\n",
    "if model == 'cnn':\n",
    "    del params['hidden_size']\n",
    "    del params['num_layers']\n",
    "elif model == 'lstm' or model == 'blstm':\n",
    "    del params['num_filters']\n",
    "    del params['filter_sizes']\n",
    "    params['embedding_size'] = params['hidden_size']\n",
    "elif model == 'clstm':\n",
    "    params['hidden_size'] = len(list(map(int, params['filter_sizes'].split(\",\")))) * params['num_filters']\n",
    "\n",
    "params_dict = sorted(params.items(), key=lambda x: x[0])\n",
    "print('Parameters:')\n",
    "for item in params_dict:\n",
    "    print('{}: {}'.format(item[0], item[1]))\n",
    "print('')\n",
    "\n",
    "# Save parameters to file\n",
    "params_file = open(os.path.join(outdir, 'params.pkl'), 'wb')\n",
    "pkl.dump(params, params_file, True)\n",
    "params_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, src_lang, trg_lang, shuffle=True):\n",
    "    with open(file_path+ src_lang+ \"_\" +trg_lang) as file:\n",
    "        sent_src_trg = file.readlines()\n",
    "    sent_src = []\n",
    "    sent_trg = []\n",
    "    for sent in sent_src_trg:\n",
    "        parts = sent.split(\" ||| \")\n",
    "        sent_src.append(parts[0])\n",
    "        sent_trg.append(parts[1])\n",
    "        \n",
    "    # Tokenizing the sentences source\n",
    "    tokens_src = []  \n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    for i in tqdm(range(0, len(sent_src))):\n",
    "        tokens = tokenizer.tokenize(sent_src[i])\n",
    "\n",
    "        tokens_src.append([src_lang+token for token in tokens])\n",
    "        \n",
    "    # Tokenizing the sentences target\n",
    "    tokens_trg = []  \n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    for i in tqdm(range(0, len(sent_trg))):\n",
    "        tokens = tokenizer.tokenize(sent_trg[i])\n",
    "\n",
    "        tokens_trg.append([trg_lang+token for token in tokens])\n",
    "\n",
    "    max_src = max([len(tokens) for tokens in tokens_src])\n",
    "    max_trg = max([len(tokens) for tokens in tokens_trg])\n",
    "    max_length = max(max_src, max_trg)\n",
    "    \n",
    "    \n",
    "    x_all = tokens_src + tokens_trg\n",
    "    vocab, vocab_dict = create_vocabulary(x_all)\n",
    "    sequences_src = []\n",
    "    for tokens in tokens_src:\n",
    "        list_ids_sub = []\n",
    "        for token in tokens:\n",
    "            #token = token.decode(\"utf-8\", errors='ignore')\n",
    "            list_ids_sub.append(vocab[token])\n",
    "        to_pad = max_src - len(tokens)\n",
    "        for index in range(0, to_pad):\n",
    "            list_ids_sub.append(len(vocab))\n",
    "        sequences_src.append(list_ids_sub)\n",
    "        \n",
    "    sequences_trg = []\n",
    "    for tokens in tokens_trg:\n",
    "        list_ids_sub = []\n",
    "        for token in tokens:\n",
    "            #token = token.decode(\"utf-8\", errors='ignore')\n",
    "            list_ids_sub.append(vocab[token])\n",
    "        to_pad = max_src - len(tokens)\n",
    "        for index in range(0, to_pad):\n",
    "            list_ids_sub.append(len(vocab))\n",
    "        sequences_trg.append(list_ids_sub)\n",
    "    pad = len(vocab)\n",
    "    vocab[\"UNK\"] = pad\n",
    "    return sequences_src, sequences_trg, vocab, max_length\n",
    "\n",
    "def create_vocabulary(x_all): #, save_path\n",
    "    vocab_dict = {}\n",
    "    for doc in x_all:\n",
    "        for token in doc:\n",
    "            #token = token.decode(\"utf-8\", errors='ignore')\n",
    "            # #print(token)\n",
    "            if token in vocab_dict:\n",
    "                vocab_dict[token] += 1\n",
    "            else:\n",
    "                vocab_dict[token] = 1\n",
    "    vocab_list = sorted(vocab_dict)\n",
    "    vocab = dict([x, y] for (y, x) in enumerate(vocab_list))\n",
    "\n",
    "    return vocab, vocab_dict\n",
    "\n",
    "def batch_iter(data, labels, max_length, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    A mini-batch iterator to generate mini-batches for training neural network\n",
    "    :param data: a list of sentences. each sentence is a vector of integers\n",
    "    :param labels: a list of labels\n",
    "    :param batch_size: the size of mini-batch\n",
    "    :param num_epochs: number of epochs\n",
    "    :return: a mini-batch iterator\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    epoch_length = data_size // batch_size\n",
    "    \n",
    "    print(\"epoch_length:\", epoch_length)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for i in range(epoch_length):\n",
    "            start_index = i * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "\n",
    "            xdata = data[start_index: end_index]\n",
    "            ydata = labels[start_index: end_index]\n",
    "            sequence_length = (end_index-start_index) *[max_length] \n",
    "\n",
    "            yield xdata, ydata, sequence_length\n",
    "            \n",
    "def load_embeddings(emb_path, vocab):\n",
    "    vocab_list = list(vocab.keys())\n",
    "    with open(emb_path) as file_model:\n",
    "        data = file_model.readlines()\n",
    "\n",
    "    model_dict = {}\n",
    "    model = np.zeros([len(vocab_list), FLAGS.embedding_size])\n",
    "    for i in tqdm(range(0, len(data))):\n",
    "        lang = data[i].split(\" \")[0].split(\"_\")[0]\n",
    "        word = data[i].split(\" \")[0]\n",
    "        vectors = [float(vector) for vector in data[i].split(\" \")[1:]]\n",
    "        model_dict.update({word: vectors})\n",
    "        if vocab_list[i] == word:\n",
    "            model[i] = vectors\n",
    "            \n",
    "    embed_dim = len(model_dict[list(model_dict.keys())[0]])\n",
    "    return model, model_dict, embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiSkipGram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_clf(object):\n",
    "    \"\"\"\n",
    "    A CNN classifier for text classification\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.max_length = config.max_length\n",
    "        self.num_classes = config.num_classes\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.filter_sizes = list(map(int, config.filter_sizes.split(\",\")))\n",
    "        self.num_filters = config.num_filters\n",
    "        self.l2_reg_lambda = config.l2_reg_lambda\n",
    "\n",
    "        # Placeholders\n",
    "        self.input_source = tf.placeholder(dtype=tf.int32, shape=[None, self.max_length])\n",
    "        self.input_target = tf.placeholder(dtype=tf.int32, shape=[None, self.max_length])\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            embedding = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], 0, 1.0), trainable= True, name=\"embedding\") # One shared embedding layer\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = embedding.assign(self.embedding_placeholder)\n",
    "            embed_source = tf.nn.embedding_lookup(embedding, self.input_source)\n",
    "            inputs_sources = tf.expand_dims(embed_source, -1)\n",
    "            \n",
    "            embed_target = tf.nn.embedding_lookup(embedding, self.input_target)\n",
    "            inputs_targets = tf.expand_dims(embed_target, -1)\n",
    "            \n",
    "\n",
    "        # Convolution & Maxpool\n",
    "        ## Source\n",
    "        pooled_outputs_source = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.get_variable(\"weights\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "                b = tf.get_variable(\"biases\", [self.num_filters], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                conv = tf.nn.conv2d(inputs_sources,\n",
    "                                    W,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name='conv')\n",
    "                # Activation function\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                # Maxpool\n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                        ksize=[1, self.max_length - filter_size + 1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding='VALID',\n",
    "                                        name='pool')\n",
    "                pooled_outputs_source.append(pooled)\n",
    "                \n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs_source, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        # Add dropout\n",
    "        h_drop_source = tf.nn.dropout(h_pool_flat, keep_prob=self.keep_prob)\n",
    "                \n",
    "        ## Target\n",
    "        pooled_outputs_target = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size, reuse=True):\n",
    "                # Convolution\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.get_variable(\"weights\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "                b = tf.get_variable(\"biases\", [self.num_filters], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                conv = tf.nn.conv2d(inputs_targets,\n",
    "                                    W,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name='conv')\n",
    "                # Activation function\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                # Maxpool\n",
    "                pooled = tf.nn.max_pool(h,\n",
    "                                        ksize=[1, self.max_length - filter_size + 1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding='VALID',\n",
    "                                        name='pool')\n",
    "                pooled_outputs_target.append(pooled)\n",
    "\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs_target, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "        # Add dropout\n",
    "        h_drop_target = tf.nn.dropout(h_pool_flat, keep_prob=self.keep_prob)\n",
    "        \n",
    "        \"\"\"#No Softmax\n",
    "        # Softmax\n",
    "        with tf.name_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([num_filters_total, self.num_classes], stddev=0.1), name='softmax_w')\n",
    "            softmax_b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name='softmax_b')\n",
    "\n",
    "            # Add L2 regularization to output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_b)\n",
    "\n",
    "            self.logits = tf.matmul(h_drop, softmax_w) + softmax_b\n",
    "            predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(predictions, 1)\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Loss\n",
    "        with tf.name_scope('loss'):\n",
    "            #losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n",
    "            losses = tf.losses.mean_squared_error(h_drop_source, h_drop_target, weights=1.0, scope=None, loss_collection=tf.GraphKeys.LOSSES)\n",
    "            # Add L2 losses\n",
    "            self.cost = tf.reduce_mean(losses) # + self.l2_reg_lambda * self.l2_loss\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_clf(object):\n",
    "    \"\"\"\"\n",
    "    LSTM and Bi-LSTM classifiers for text classification\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.max_length = config.max_length\n",
    "        self.num_classes = config.num_classes\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.l2_reg_lambda = config.l2_reg_lambda\n",
    "\n",
    "        # Placeholders\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.input_source = tf.placeholder(dtype=tf.int32, shape=[None, self.max_length])\n",
    "        self.input_target = tf.placeholder(dtype=tf.int32, shape=[None, self.max_length])\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.sequence_length = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "        # L2 loss\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Word embedding\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            embedding = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], 0, 1.0), trainable= True, name=\"embedding\") # One shared embedding layer\n",
    "            self.embedding_placeholder = tf.placeholder(tf.float32, [self.vocab_size, self.embedding_size])\n",
    "            self.embedding_init = embedding.assign(self.embedding_placeholder)\n",
    "            embed_source = tf.nn.embedding_lookup(embedding, self.input_source)\n",
    "            inputs_sources = tf.expand_dims(embed_source, -1)\n",
    "\n",
    "            embed_target = tf.nn.embedding_lookup(embedding, self.input_target)\n",
    "            inputs_targets = tf.expand_dims(embed_target, -1)\n",
    "\n",
    "        # Input dropout\n",
    "        self.input_source = tf.nn.dropout(inputs_sources, keep_prob=self.keep_prob)\n",
    "        self.input_target = tf.nn.dropout(inputs_targets, keep_prob=self.keep_prob)\n",
    "\n",
    "        # LSTM\n",
    "        if config.clf == 'lstm':\n",
    "            self.final_state = self.normal_lstm()\n",
    "        else:\n",
    "            self.final_state = self.bi_lstm()\n",
    "        \n",
    "        \"\"\" \n",
    "        # Softmax output layer\n",
    "        with tf.name_scope('softmax'):\n",
    "            # softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            if config.clf == 'lstm':\n",
    "                softmax_w = tf.get_variable('softmax_w', shape=[self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            else:\n",
    "                softmax_w = tf.get_variable('softmax_w', shape=[2 * self.hidden_size, self.num_classes], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable('softmax_b', shape=[self.num_classes], dtype=tf.float32)\n",
    "\n",
    "            # L2 regularization for output layer\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_w)\n",
    "            self.l2_loss += tf.nn.l2_loss(softmax_b)\n",
    "\n",
    "            # self.logits = tf.matmul(self.final_state[self.num_layers - 1].h, softmax_w) + softmax_b\n",
    "            if config.clf == 'lstm':\n",
    "                self.logits = tf.matmul(self.final_state[self.num_layers - 1].h, softmax_w) + softmax_b\n",
    "            else:\n",
    "                self.logits = tf.matmul(self.final_state, softmax_w) + softmax_b\n",
    "            predictions = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(predictions, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Loss\n",
    "        with tf.name_scope('loss'):\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            # L2 regularization for LSTM weights\n",
    "            for tv in tvars:\n",
    "                if 'kernel' in tv.name:\n",
    "                    self.l2_loss += tf.nn.l2_loss(tv)\n",
    "\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.input_y,\n",
    "                                                                    logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, self.input_y)\n",
    "            self.correct_num = tf.reduce_sum(tf.cast(correct_predictions, tf.float32))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "    def normal_lstm(self):\n",
    "        # LSTM Cell\n",
    "        cell = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                       forget_bias=1.0,\n",
    "                                       state_is_tuple=True,\n",
    "                                       reuse=tf.get_variable_scope().reuse)\n",
    "        # Add dropout to cell output\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers, state_is_tuple=True)\n",
    "\n",
    "        self._initial_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic LSTM\n",
    "        with tf.variable_scope('LSTM'):\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell,\n",
    "                                               inputs=self.inputs,\n",
    "                                               initial_state=self._initial_state,\n",
    "                                               sequence_length=self.sequence_length)\n",
    "\n",
    "        final_state = state\n",
    "\n",
    "        return final_state\n",
    "\n",
    "\n",
    "    def bi_lstm(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.hidden_size,\n",
    "                                          forget_bias=1.0,\n",
    "                                          state_is_tuple=True,\n",
    "                                          reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "        # Add dropout to cell output\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.keep_prob)\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.keep_prob)\n",
    "\n",
    "        # Stacked LSTMs\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw] * self.num_layers, state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw] * self.num_layers, state_is_tuple=True)\n",
    "\n",
    "        self._initial_state_fw = cell_fw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self._initial_state_bw = cell_bw.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        # Dynamic Bi-LSTM\n",
    "        with tf.variable_scope('Bi-LSTM'):\n",
    "            _, state = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                       cell_bw,\n",
    "                                                       inputs=self.inputs,\n",
    "                                                       initial_state_fw=self._initial_state_fw,\n",
    "                                                       initial_state_bw=self._initial_state_bw,\n",
    "                                                       sequence_length=self.sequence_length)\n",
    "\n",
    "        state_fw = state[0]\n",
    "        state_bw = state[1]\n",
    "        output = tf.concat([state_fw[self.num_layers - 1].h, state_bw[self.num_layers - 1].h], 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 35230.67it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 22834.32it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5313.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "epoch_length: 1\n",
      "2018-05-25T15:14:28.081260: step: 1, loss: 9.61059\n",
      "2018-05-25T15:14:28.201663: step: 2, loss: 5.85256\n",
      "2018-05-25T15:14:28.325846: step: 3, loss: 3.62878\n",
      "2018-05-25T15:14:28.486504: step: 4, loss: 2.00446\n",
      "2018-05-25T15:14:28.639046: step: 5, loss: 1.13188\n",
      "2018-05-25T15:14:28.756389: step: 6, loss: 0.644091\n",
      "2018-05-25T15:14:28.886344: step: 7, loss: 0.327571\n",
      "2018-05-25T15:14:29.011215: step: 8, loss: 0.121332\n",
      "2018-05-25T15:14:29.140364: step: 9, loss: 0.0567066\n",
      "2018-05-25T15:14:29.294642: step: 10, loss: 0.0307281\n",
      "2018-05-25T15:14:29.442458: step: 11, loss: 0.0129232\n",
      "2018-05-25T15:14:29.564117: step: 12, loss: 0.00420781\n",
      "2018-05-25T15:14:29.690745: step: 13, loss: 0.00167947\n",
      "2018-05-25T15:14:29.809941: step: 14, loss: 0.00149764\n",
      "2018-05-25T15:14:29.931064: step: 15, loss: 1.63307e-06\n",
      "2018-05-25T15:14:30.070684: step: 16, loss: 0\n",
      "2018-05-25T15:14:30.258684: step: 17, loss: 0\n",
      "2018-05-25T15:14:30.373956: step: 18, loss: 0\n",
      "2018-05-25T15:14:30.498939: step: 19, loss: 0\n",
      "2018-05-25T15:14:30.623775: step: 20, loss: 0\n",
      "2018-05-25T15:14:30.753554: step: 21, loss: 0\n",
      "2018-05-25T15:14:30.902154: step: 22, loss: 0\n",
      "2018-05-25T15:14:31.056998: step: 23, loss: 0\n",
      "2018-05-25T15:14:31.171506: step: 24, loss: 0\n",
      "2018-05-25T15:14:31.295929: step: 25, loss: 0\n",
      "2018-05-25T15:14:31.415179: step: 26, loss: 0\n",
      "2018-05-25T15:14:31.539592: step: 27, loss: 0\n",
      "2018-05-25T15:14:31.735181: step: 28, loss: 0\n",
      "2018-05-25T15:14:31.864794: step: 29, loss: 0\n",
      "2018-05-25T15:14:31.979440: step: 30, loss: 0\n",
      "2018-05-25T15:14:32.122454: step: 31, loss: 0\n",
      "2018-05-25T15:14:32.278282: step: 32, loss: 0\n",
      "2018-05-25T15:14:32.407122: step: 33, loss: 0\n",
      "2018-05-25T15:14:32.519059: step: 34, loss: 0\n",
      "2018-05-25T15:14:32.636933: step: 35, loss: 0\n",
      "2018-05-25T15:14:32.778490: step: 36, loss: 0\n",
      "2018-05-25T15:14:32.944720: step: 37, loss: 0\n",
      "2018-05-25T15:14:33.059294: step: 38, loss: 0\n",
      "2018-05-25T15:14:33.179885: step: 39, loss: 0\n",
      "2018-05-25T15:14:33.298172: step: 40, loss: 0\n",
      "2018-05-25T15:14:33.420302: step: 41, loss: 0\n",
      "2018-05-25T15:14:33.561128: step: 42, loss: 0\n",
      "2018-05-25T15:14:33.715789: step: 43, loss: 0\n",
      "2018-05-25T15:14:33.830384: step: 44, loss: 0\n",
      "2018-05-25T15:14:33.957271: step: 45, loss: 0\n",
      "2018-05-25T15:14:34.079075: step: 46, loss: 0\n",
      "2018-05-25T15:14:34.209390: step: 47, loss: 0\n",
      "2018-05-25T15:14:34.322265: step: 48, loss: 0\n",
      "2018-05-25T15:14:34.440733: step: 49, loss: 0\n",
      "2018-05-25T15:14:34.561174: step: 50, loss: 0\n",
      "\n",
      "All the files have been saved to /Users/meryemmhamdi/Desktop/rig1/home/meryem/meryem/multilingual/MultiEmb/runs/1527253393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and save data\n",
    "# =============================================================================\n",
    "\n",
    "sources, targets, vocab, max_length = load_data(FLAGS.data_file, \"fr\", \"en\", shuffle=True)\n",
    "\n",
    "FLAGS.vocab_size = len(vocab)\n",
    "FLAGS.max_length = max_length\n",
    "\n",
    "embedding, emb_dict, FLAGS.embedding_size = load_embeddings(FLAGS.emb_path, vocab)\n",
    "\n",
    "# Simple Cross validation\n",
    "# TODO use k-fold cross validation\n",
    "train_src, valid_src, train_trg, valid_trg = train_test_split(sources, targets, test_size=FLAGS.test_size, random_state=22)\n",
    "\n",
    "\n",
    "# Batch iterator\n",
    "#FLAGS.batch_size\n",
    "train_data = batch_iter(train_src, train_trg, max_length, 10, FLAGS.num_epochs)\n",
    "\n",
    "# Train\n",
    "# =============================================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        if FLAGS.clf == 'cnn':\n",
    "            classifier = cnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'lstm' or FLAGS.clf == 'blstm':\n",
    "            classifier = rnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'clstm':\n",
    "            classifier = clstm_clf(FLAGS)\n",
    "        else:\n",
    "            raise ValueError('clf should be one of [cnn, lstm, blstm, clstm]')\n",
    "\n",
    "        # Train procedure\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Summaries\n",
    "        loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "        #accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "        # Train summary\n",
    "        train_summary_op = tf.summary.merge_all()\n",
    "        train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summary\n",
    "        valid_summary_op = tf.summary.merge_all()\n",
    "        valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "        valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=FLAGS.num_checkpoint)\n",
    "\n",
    "        sess.run(classifier.embedding_init, feed_dict={classifier.embedding_placeholder: embedding})\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "            input_src, input_trg, sequence_length = input_data\n",
    "\n",
    "            fetches = {'step': global_step,\n",
    "                       'cost': classifier.cost}#,\n",
    "                       #'accuracy': classifier.accuracy}\n",
    "            feed_dict = {classifier.input_source: input_src,\n",
    "                         classifier.input_target: input_trg}\n",
    "\n",
    "            if FLAGS.clf != 'cnn':\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_src)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "            if is_training:\n",
    "                fetches['train_op'] = train_op\n",
    "                fetches['summaries'] = train_summary_op\n",
    "                feed_dict[classifier.keep_prob] = FLAGS.keep_prob\n",
    "            else:\n",
    "                fetches['summaries'] = valid_summary_op\n",
    "                feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "            vars = sess.run(fetches, feed_dict)\n",
    "            step = vars['step']\n",
    "            cost = vars['cost']\n",
    "            #accuracy = vars['accuracy']\n",
    "            summaries = vars['summaries']\n",
    "\n",
    "            # Write summaries to file\n",
    "            if is_training:\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "            else:\n",
    "                valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step: {}, loss: {:g}\".format(time_str, step, cost))\n",
    "\n",
    "\n",
    "        print('Start training ...')\n",
    "\n",
    "        for train_input in train_data:\n",
    "            #print(train_input)\n",
    "            run_step(train_input, is_training=True)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            if current_step % FLAGS.evaluate_every_steps == 0:\n",
    "                print('\\nValidation')\n",
    "                run_step((x_valid, y_valid, valid_lengths), is_training=False)\n",
    "                print('')\n",
    "\n",
    "            if current_step % FLAGS.save_every_steps == 0:\n",
    "                save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "        print('\\nAll the files have been saved to {}\\n'.format(outdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': <tf.Tensor 'accuracy/accuracy:0' shape=() dtype=float32>,\n",
       " 'correct_num': <tf.Tensor 'accuracy/Sum:0' shape=() dtype=float32>,\n",
       " 'cost': <tf.Tensor 'loss/add:0' shape=() dtype=float32>,\n",
       " 'embedding_size': 256,\n",
       " 'filter_sizes': [3, 4, 5],\n",
       " 'input_x': <tf.Tensor 'Placeholder:0' shape=(?, 62) dtype=int32>,\n",
       " 'input_y': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int64>,\n",
       " 'keep_prob': <tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>,\n",
       " 'l2_loss': <tf.Tensor 'softmax/add_1:0' shape=() dtype=float32>,\n",
       " 'l2_reg_lambda': 0.001,\n",
       " 'logits': <tf.Tensor 'softmax/add_2:0' shape=(?, 2) dtype=float32>,\n",
       " 'max_length': 62,\n",
       " 'num_classes': 2,\n",
       " 'num_filters': 128,\n",
       " 'predictions': <tf.Tensor 'softmax/ArgMax:0' shape=(?,) dtype=int64>,\n",
       " 'vocab_size': 377}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
