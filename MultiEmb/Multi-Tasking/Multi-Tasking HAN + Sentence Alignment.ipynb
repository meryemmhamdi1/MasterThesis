{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Little Tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Placeholders For X And Y (for feeding in data)\n",
    "X = tf.placeholder(\"float\",[10, 10],name=\"X\") # Our input is 10x10\n",
    "Y = tf.placeholder(\"float\", [10, 1],name=\"Y\") # Our output is 10x1\n",
    "# Create a Trainable Variable, \"W\", our weights for the linear transformation\n",
    "initial_W = np.zeros((10,1))\n",
    "W = tf.Variable(initial_W, name=\"W\", dtype=\"float32\")\n",
    "\n",
    "# Define Your Loss Function\n",
    "Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses\n",
      "[[ 0.57452118]\n",
      " [ 0.04220407]\n",
      " [ 0.11925153]\n",
      " [ 0.16426601]\n",
      " [ 0.02727749]\n",
      " [ 0.03581695]\n",
      " [ 0.25064379]\n",
      " [ 0.07625095]\n",
      " [ 0.98142004]\n",
      " [ 0.79552728]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # set up the session\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    Model_Loss = sess.run(\n",
    "                Loss, # the first argument is the name of the Tensorflow variabl you want to return\n",
    "                { # the second argument is the data for the placeholders\n",
    "                  X: np.random.rand(10,10),\n",
    "                  Y: np.random.rand(10).reshape(-1,1)\n",
    "                })\n",
    "    print(Model_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Task Case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the Placeholders\n",
    "X = tf.placeholder(\"float\", [10, 10], name=\"X\")\n",
    "Y1 = tf.placeholder(\"float\", [10, 20], name=\"Y1\")\n",
    "Y2 = tf.placeholder(\"float\", [10, 20], name=\"Y2\")\n",
    "\n",
    "# Define the weights for the layers\n",
    "\n",
    "initial_shared_layer_weights = np.random.rand(10,20)\n",
    "initial_Y1_layer_weights = np.random.rand(20,20)\n",
    "initial_Y2_layer_weights = np.random.rand(20,20)\n",
    "\n",
    "shared_layer_weights = tf.Variable(initial_shared_layer_weights, name=\"share_W\", dtype=\"float32\")\n",
    "Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name=\"share_Y1\", dtype=\"float32\")\n",
    "Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name=\"share_Y2\", dtype=\"float32\")\n",
    "\n",
    "# Construct the Layers with RELU Activations\n",
    "shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))\n",
    "Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))\n",
    "Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))\n",
    "\n",
    "# Calculate Loss\n",
    "Y1_Loss = tf.nn.l2_loss(Y1-Y1_layer)\n",
    "Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)\n",
    "\n",
    "Joint_Loss = Y1_Loss + Y2_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.89154e+06\n",
      "5.85761e+06\n",
      "6.87897e+06\n",
      "7.56048e+06\n",
      "8.18204e+06\n",
      "6.5788e+06\n",
      "7.81574e+06\n",
      "7.29146e+06\n",
      "8.12310e+06\n",
      "6.57724e+06\n"
     ]
    }
   ],
   "source": [
    "# Calculation (Session) Code\n",
    "# ==========================\n",
    "\n",
    "# open the session\n",
    "\n",
    "# optimisers\n",
    "Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)\n",
    "Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    for iters in range(10):\n",
    "        if np.random.rand() < 0.5:\n",
    "            _, Y1_loss = session.run([Y1_op, Y1_Loss],\n",
    "                            {\n",
    "                              X: np.random.rand(10,10)*10,\n",
    "                              Y1: np.random.rand(10,20)*10,\n",
    "                              Y2: np.random.rand(10,20)*10\n",
    "                              })\n",
    "            print(Y1_loss)\n",
    "        else:\n",
    "            _, Y2_loss = session.run([Y2_op, Y2_Loss],\n",
    "                            {\n",
    "                              X: np.random.rand(10,10)*10,\n",
    "                              Y1: np.random.rand(10,20)*10,\n",
    "                              Y2: np.random.rand(10,20)*10\n",
    "                              })\n",
    "            print(Y2_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training at the Same time: Joint Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11279e+07\n"
     ]
    }
   ],
   "source": [
    "Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    _, Joint_Loss = session.run([Optimiser, Joint_Loss],\n",
    "                    {\n",
    "                      X: np.random.rand(10,10)*10,\n",
    "                      Y1: np.random.rand(10,20)*10,\n",
    "                      Y2: np.random.rand(10,20)*10\n",
    "                      })\n",
    "    print(Joint_Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Task ONE:\n",
    "Training multilingual embeddings (initialized with the embeddings aligned offline or not) using concatenation of parallel sentences in fr-en, it-en, de-en where the weights of LSTM-CNN representation of the sentences are shared with Task TWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Task TWO:\n",
    "Training hierarchical representation of documents using CNN+LSTM (document <- sentence <- word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01003566 -0.00049934]\n",
      " [ 0.00282546  0.00468037]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import data_util\n",
    "from model_components import task_specific_attention, bidirectional_rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "class MultiTaskEmbeddingsHANClassifier():\n",
    "    \"\"\" Implementation of Multi-Tasking of the training of alignment of multilingual embeddings \n",
    "    and crosslingual document classification model described in `Hierarchical Attention Networks for Document Classification (Yang et al., 2016)`\n",
    "    (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "           vocab_size,\n",
    "           embedding_size,\n",
    "           classes,\n",
    "           word_cell,\n",
    "           sentence_cell,\n",
    "           word_output_size,\n",
    "           sentence_output_size,\n",
    "           max_grad_norm,\n",
    "           dropout_keep_proba,\n",
    "           is_training=None,\n",
    "           learning_rate=1e-4,\n",
    "           device='/cpu:0',\n",
    "           scope=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.classes = classes\n",
    "        self.word_cell = word_cell\n",
    "        self.word_output_size = word_output_size\n",
    "        self.sentence_cell = sentence_cell\n",
    "        self.sentence_output_size = sentence_output_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.dropout_keep_proba = dropout_keep_proba\n",
    "\n",
    "        with tf.variable_scope(scope or 'tcm') as scope:\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        if is_training is not None:\n",
    "            self.is_training = is_training\n",
    "        else:\n",
    "            self.is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "        self.sample_weights = tf.placeholder(shape=(None,), dtype=tf.float32, name='sample_weights')\n",
    "\n",
    "        # [document x sentence x word]\n",
    "        self.inputs = tf.placeholder(shape=(None, None, None), dtype=tf.int32, name='inputs')\n",
    "\n",
    "        # [document x sentence]\n",
    "        self.word_lengths = tf.placeholder(shape=(None, None), dtype=tf.int32, name='word_lengths')\n",
    "\n",
    "        # [document]\n",
    "        self.sentence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='sentence_lengths')\n",
    "\n",
    "        # [document]\n",
    "        self.labels = tf.placeholder(shape=(None,), dtype=tf.int32, name='labels')\n",
    "\n",
    "        (self.document_size,\n",
    "         self.sentence_size,\n",
    "         self.word_size) = tf.unstack(tf.shape(self.inputs))\n",
    "\n",
    "        self._init_embedding(scope)\n",
    "\n",
    "        # embeddings cannot be placed on GPU\n",
    "        with tf.device(device):\n",
    "            self._init_body(scope)\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.multiply(self.cross_entropy, self.sample_weights))\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.labels, 1), tf.float32))\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            grads, global_norm = tf.clip_by_global_norm(\n",
    "                tf.gradients(self.loss, tvars),\n",
    "                self.max_grad_norm)\n",
    "            tf.summary.scalar('global_grad_norm', global_norm)\n",
    "\n",
    "            class_opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "            self.train_class_opt = class_opt.apply_gradients(\n",
    "                zip(grads, tvars), name='train_op',\n",
    "                global_step=self.global_step)\n",
    "\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def _init_embedding(self, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "              with tf.variable_scope(\"embedding\") as scope:\n",
    "                self.embedding_matrix = tf.get_variable(\n",
    "                  name=\"embedding_matrix\",\n",
    "                  shape=[self.vocab_size, self.embedding_size],\n",
    "                  initializer=layers.xavier_initializer(),\n",
    "                  dtype=tf.float32)\n",
    "                self.inputs_embedded = tf.nn.embedding_lookup(\n",
    "                  self.embedding_matrix, self.inputs)\n",
    "\n",
    "    def _init_body(self, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            word_level_inputs = tf.reshape(self.inputs_embedded, [\n",
    "                self.document_size * self.sentence_size,\n",
    "                self.word_size,\n",
    "                self.embedding_size\n",
    "            ])\n",
    "            word_level_lengths = tf.reshape(\n",
    "                self.word_lengths, [self.document_size * self.sentence_size])\n",
    "\n",
    "        with tf.variable_scope('word') as scope:\n",
    "            word_encoder_output, _ = bidirectional_rnn(\n",
    "                self.word_cell, self.word_cell,\n",
    "                word_level_inputs, word_level_lengths,\n",
    "                scope=scope)\n",
    "        \n",
    "        with tf.variable_scope('attention') as scope:\n",
    "            word_level_output = task_specific_attention(\n",
    "                word_encoder_output,\n",
    "                self.word_output_size,\n",
    "                scope=scope)\n",
    "\n",
    "        with tf.variable_scope('dropout'):\n",
    "            word_level_output = layers.dropout(\n",
    "                word_level_output, keep_prob=self.dropout_keep_proba,\n",
    "                is_training=self.is_training,\n",
    "          )\n",
    "    \n",
    "        # sentence_level\n",
    "        sentence_inputs = tf.reshape(\n",
    "        word_level_output, [self.document_size, self.sentence_size, self.word_output_size])\n",
    "\n",
    "        with tf.variable_scope('sentence') as scope:\n",
    "            sentence_encoder_output, _ = bidirectional_rnn(\n",
    "                self.sentence_cell, self.sentence_cell, sentence_inputs, self.sentence_lengths, scope=scope)\n",
    "\n",
    "        with tf.variable_scope('attention', reuse=True) as scope:\n",
    "            sentence_level_output = task_specific_attention(\n",
    "                sentence_encoder_output, self.sentence_output_size, scope=scope)\n",
    "\n",
    "        with tf.variable_scope('dropout'):\n",
    "            sentence_level_output = layers.dropout(\n",
    "                sentence_level_output, keep_prob=self.dropout_keep_proba,\n",
    "                is_training=self.is_training,\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('classifier'):\n",
    "            self.logits = layers.fully_connected(\n",
    "                sentence_level_output, self.classes, activation_fn=None)\n",
    "\n",
    "            self.prediction = tf.argmax(self.logits, axis=-1)\n",
    "\n",
    "    def get_feed_data(self, x, y=None, class_weights=None, is_training=True):\n",
    "        x_m, doc_sizes, sent_sizes = data_util.batch(x)\n",
    "        fd = {\n",
    "            self.inputs: x_m,\n",
    "            self.sentence_lengths: doc_sizes,\n",
    "            self.word_lengths: sent_sizes,\n",
    "        }\n",
    "        if y is not None:\n",
    "            fd[self.labels] = y\n",
    "            if class_weights is not None:\n",
    "                fd[self.sample_weights] = [class_weights[yy] for yy in y]\n",
    "            else:\n",
    "                fd[self.sample_weights] = np.ones(shape=[len(x_m)], dtype=np.float32)\n",
    "        fd[self.is_training] = is_training\n",
    "        return fd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tasking the two tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n",
    "    except ImportError:\n",
    "        LSTMCell = tf.nn.rnn_cell.LSTMCell\n",
    "        LSTMStateTuple = tf.nn.rnn_cell.LSTMStateTuple\n",
    "        GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "        model = MultiTaskEmbeddingsHANClassifier(\n",
    "            vocab_size=10,\n",
    "            embedding_size=5,\n",
    "            classes=2,\n",
    "            word_cell=GRUCell(10),\n",
    "            sentence_cell=GRUCell(10),\n",
    "            word_output_size=10,\n",
    "            sentence_output_size=10,\n",
    "            max_grad_norm=5.0,\n",
    "            dropout_keep_proba=0.5,\n",
    "        )\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    fd = {\n",
    "        model.is_training: False,\n",
    "        model.doc_inputs: [[\n",
    "            [5, 4, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [6, 7, 0, 0]\n",
    "        ],\n",
    "        [\n",
    "            [2, 2, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [0, 0, 0, 0]\n",
    "        ]],\n",
    "        model.source_sentences: [\n",
    "            [5, 4, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [6, 7, 0, 0],\n",
    "            [2, 2, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [0, 0, 0, 0]\n",
    "        ],\n",
    "        model.target_sentences: [\n",
    "            [5, 4, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [6, 7, 0, 0],\n",
    "            [2, 2, 1, 0],\n",
    "            [3, 3, 6, 7],\n",
    "            [0, 0, 0, 0]\n",
    "        ],\n",
    "        model.word_lengths: [\n",
    "            [3, 4, 2],\n",
    "            [3, 4, 0],\n",
    "        ],\n",
    "        model.sentence_lengths: [3, 2],\n",
    "        model.labels: [0, 1],\n",
    "        model.sample_weights:[1,1],\n",
    "    }\n",
    "    \n",
    "    for iters in range(100): #number of minibatches\n",
    "        if np.random.rand() < 0.5:\n",
    "            _, Y1_loss = session.run([Y1_op, Y1_Loss], fd)\n",
    "        else:\n",
    "            _, Y1_loss = session.run([Y1_op, Y1_Loss], fd)\n",
    "            \n",
    "\n",
    "    print(session.run(model.logits, fd))\n",
    "    session.run(model.train_op, fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
